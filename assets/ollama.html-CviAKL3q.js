import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,a as s,o}from"./app-DFbPUm1L.js";const t="/assets/images/windows/ollama/ollama.png",i="/assets/svg/deepseek.svg",n="/assets/images/windows/ollama/o-1.png",p="/assets/images/windows/ollama/o-2.png",r="/assets/images/windows/ollama/o-3.png",c="/assets/images/windows/ollama/o-4.png",g="/assets/images/windows/ollama/o-5.png",m="/assets/images/windows/ollama/o-6.png",d="/assets/images/windows/ollama/o-7.png",h={};function f(u,a){return o(),l("div",null,a[0]||(a[0]=[s('<figure><img src="'+t+'" alt="Ollama Logo" tabindex="0" loading="lazy"><figcaption>Ollama Logo</figcaption></figure><p>Ollama是一个创新的平台，旨在简化人工智能模型的使用与部署。用户可以轻松地下载、运行和管理各种AI模型，而无需深入的技术背景。Ollama提供了友好的用户界面，使得开发者和研究人员能够快速实现他们的想法，并将AI应用于实际项目中。无论是文本生成、图像处理还是其他机器学习任务，Ollama都能提供高效的解决方案，助力用户在AI领域的探索与实践。</p><hr><figure><img src="'+i+'" alt="Deepseek Logo" tabindex="0" loading="lazy"><figcaption>Deepseek Logo</figcaption></figure><p>无需多言，<strong>2025-01-20</strong> Deepseek开源R1模型，以低成本获得对标o1的性能，最重要是 <strong>开源</strong>， <strong>开源</strong>， <strong>开源</strong>！</p><hr><p>由于准备在主力机Windows上运行大模型的显卡，以下介绍Windows的安装方案。实际上，各个安装方案差距很小，Ollama也支持Docker部署，几乎实现了傻瓜式操作。</p><h2 id="🛠️-安装并运行ollama" tabindex="-1"><a class="header-anchor" href="#🛠️-安装并运行ollama"><span>🛠️ 安装并运行Ollama</span></a></h2><p>访问<a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama官网</a>下载对应系统的安装包。</p><blockquote><p>⚠️ <strong>注意</strong>：Windows的安装包一点击就立刻自动安装到C盘，无法修改路径。</p></blockquote><p>安装完成后，安装窗口会自动消失，可能会让人怀疑是否安装成功。不过可以在Windows开始菜单的最近安装项目中找到Ollama。</p><figure><img src="'+n+'" alt="开始菜单最近安装项目" tabindex="0" loading="lazy"><figcaption>开始菜单最近安装项目</figcaption></figure><p>点击运行Ollama，经过短暂的加载后不会有什么反应，没有交互界面，但在右下角托盘中可以看到Ollama的图标。</p><figure><img src="'+p+'" alt="托盘菜单Ollama正在运行" tabindex="0" loading="lazy"><figcaption>托盘菜单Ollama正在运行</figcaption></figure><p>此时打开cmd，输入<code>ollama</code>，若输出help相关信息则证明已安装成功。</p><figure><img src="'+r+'" alt="CMD中运行ollama" tabindex="0" loading="lazy"><figcaption>CMD中运行ollama</figcaption></figure><hr><h2 id="📥-下载并运行deepseek-r1模型" tabindex="-1"><a class="header-anchor" href="#📥-下载并运行deepseek-r1模型"><span>📥 下载并运行Deepseek R1模型</span></a></h2><p>访问<a href="https://ollama.com/search" target="_blank" rel="noopener noreferrer">Ollama Models列表</a>，搜索Deepseek R1模型，并访问详情页。</p><figure><img src="'+c+'" alt="Ollama Deepseek R1 模型页面" tabindex="0" loading="lazy"><figcaption>Ollama Deepseek R1 模型页面</figcaption></figure><p>在下方tags中可以看到模型不同params大小的版本以及所需的显存（或内存）大小。</p><figure><img src="'+g+'" alt="Deepseek R1 不同版本及显存或内存需求" tabindex="0" loading="lazy"><figcaption>Deepseek R1 不同版本及显存或内存需求</figcaption></figure><div class="hint-container warning"><p class="hint-container-title">注意</p><p><strong>注意</strong>：Deepseek R1实际上并没有14b的版本，只有671b的原版。tags中显示的7b、8b等其他版本均为通过Qwen模型进行提炼的更小版本。</p></div><div class="hint-container tip"><p class="hint-container-title">提示</p><p>大模型在推理时，会将参数（param）加载到显存或内存中。如果计算机配备了高性能显卡进行推理，将参数加载到显存中比加载到内存中进行推理要更快。</p></div><p>我的显卡是 <strong>RTX 4070 SUPER</strong>，配备 <strong>12GB</strong> 显存，因此选择 <code>14b</code> 的版本进行下载。回到CMD，在CMD中运行如下命令，随后Ollama将开始下载对应的模型：</p><div class="language-bash" data-highlighter="shiki" data-ext="bash" data-title="bash" style="background-color:#1E1E1E;color:#D4D4D4;"><pre class="shiki dark-plus vp-code"><code><span class="line"><span style="color:#DCDCAA;">ollama</span><span style="color:#CE9178;"> pull</span><span style="color:#CE9178;"> deepseek-r1:14b</span></span></code></pre></div><figure><img src="'+m+'" alt="Ollama拉取Deepseek R1模型" tabindex="0" loading="lazy"><figcaption>Ollama拉取Deepseek R1模型</figcaption></figure><p>根据控制台打印的信息，使用 <code>ollama run</code> 命令运行模型：</p><div class="language-bash" data-highlighter="shiki" data-ext="bash" data-title="bash" style="background-color:#1E1E1E;color:#D4D4D4;"><pre class="shiki dark-plus vp-code"><code><span class="line"><span style="color:#DCDCAA;">ollama</span><span style="color:#CE9178;"> run</span><span style="color:#CE9178;"> deepseek-r1:14b</span></span></code></pre></div><figure><img src="'+d+'" alt="运行结果" tabindex="0" loading="lazy"><figcaption>运行结果</figcaption></figure><p>速度比预期快很多，4070S显卡能达到每秒 <strong>20个token</strong> 的生成速度，非常流畅。</p><p>但是本地部署的毕竟是蒸馏模型，有机会还是可以购买Deepseek的API，价格比之Openai的API优惠不知道多少（2025-02-07），满血模型的性能比蒸馏模型性能高好几个档次。</p><h2 id="api调用" tabindex="-1"><a class="header-anchor" href="#api调用"><span>API调用</span></a></h2><p>Ollama支持RESTful api调用，详见<a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank" rel="noopener noreferrer">官方文档</a></p>',34)]))}const k=e(h,[["render",f]]),O=JSON.parse(`{"path":"/win_linux/ollama.html","title":"Ollama本地部署Deepseek R1","lang":"zh-CN","frontmatter":{"title":"Ollama本地部署Deepseek R1","order":1,"category":["Windows"],"tag":["ollama"],"description":"Ollama LogoOllama Logo Ollama是一个创新的平台，旨在简化人工智能模型的使用与部署。用户可以轻松地下载、运行和管理各种AI模型，而无需深入的技术背景。Ollama提供了友好的用户界面，使得开发者和研究人员能够快速实现他们的想法，并将AI应用于实际项目中。无论是文本生成、图像处理还是其他机器学习任务，Ollama都能提供高效的解...","head":[["meta",{"property":"og:url","content":"https://jayczee.cn/win_linux/ollama.html"}],["meta",{"property":"og:site_name","content":"Jayczee's Blog"}],["meta",{"property":"og:title","content":"Ollama本地部署Deepseek R1"}],["meta",{"property":"og:description","content":"Ollama LogoOllama Logo Ollama是一个创新的平台，旨在简化人工智能模型的使用与部署。用户可以轻松地下载、运行和管理各种AI模型，而无需深入的技术背景。Ollama提供了友好的用户界面，使得开发者和研究人员能够快速实现他们的想法，并将AI应用于实际项目中。无论是文本生成、图像处理还是其他机器学习任务，Ollama都能提供高效的解..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://jayczee.cn/assets/images/windows/ollama/ollama.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-05T10:13:42.000Z"}],["meta",{"property":"article:tag","content":"ollama"}],["meta",{"property":"article:modified_time","content":"2025-03-05T10:13:42.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Ollama本地部署Deepseek R1\\",\\"image\\":[\\"https://jayczee.cn/assets/images/windows/ollama/ollama.png\\",\\"https://jayczee.cn/assets/svg/deepseek.svg\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-1.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-2.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-3.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-4.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-5.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-6.png\\",\\"https://jayczee.cn/assets/images/windows/ollama/o-7.png\\"],\\"dateModified\\":\\"2025-03-05T10:13:42.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Jayczee\\",\\"url\\":\\"https://jayczee.cn\\"}]}"]]},"headers":[{"level":2,"title":"🛠️ 安装并运行Ollama","slug":"🛠️-安装并运行ollama","link":"#🛠️-安装并运行ollama","children":[]},{"level":2,"title":"📥 下载并运行Deepseek R1模型","slug":"📥-下载并运行deepseek-r1模型","link":"#📥-下载并运行deepseek-r1模型","children":[]},{"level":2,"title":"API调用","slug":"api调用","link":"#api调用","children":[]}],"git":{"createdTime":1738909055000,"updatedTime":1741169622000,"contributors":[{"name":"Jayczee","username":"Jayczee","email":"jayczee@yeah.net","commits":2,"url":"https://github.com/Jayczee"}]},"readingTime":{"minutes":2.84,"words":852},"filePathRelative":"win_linux/ollama.md","localizedDate":"2025年2月7日","excerpt":"<figure><img src=\\"/assets/images/windows/ollama/ollama.png\\" alt=\\"Ollama Logo\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>Ollama Logo</figcaption></figure>\\n<p>Ollama是一个创新的平台，旨在简化人工智能模型的使用与部署。用户可以轻松地下载、运行和管理各种AI模型，而无需深入的技术背景。Ollama提供了友好的用户界面，使得开发者和研究人员能够快速实现他们的想法，并将AI应用于实际项目中。无论是文本生成、图像处理还是其他机器学习任务，Ollama都能提供高效的解决方案，助力用户在AI领域的探索与实践。</p>","autoDesc":true}`);export{k as comp,O as data};
